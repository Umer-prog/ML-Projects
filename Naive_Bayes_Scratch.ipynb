{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b44e9c",
   "metadata": {},
   "source": [
    "### Naive Bayes From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab3b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf526bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes empty dictionaries to store:\n",
    "        - priors: probability of each class P(C)\n",
    "        - means: mean of each feature for each class\n",
    "        - variances: variance of each feature for each class\n",
    "        \"\"\"\n",
    "        self.class_priors = {}  # P(C) for each class\n",
    "        self.means = {}        # Mean of features per class\n",
    "        self.variances = {}    # Variance of features per class\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the Naive Bayes classifier.\n",
    "        Parameters:\n",
    "        X : numpy array of shape (m, n) - Training features\n",
    "        y : numpy array of shape (m,)   - Training labels\n",
    "        \"\"\"\n",
    "        m, n = X.shape  # m: number of samples, n: number of features\n",
    "        classes = np.unique(y)  # All unique class labels\n",
    "        \n",
    "        for cls in classes:\n",
    "            # Get all rows where label == cls\n",
    "            X_cls = X[y == cls]\n",
    "            \n",
    "            # Prior probability P(C) = (# samples in class) / (total samples)\n",
    "            self.class_priors[cls] = X_cls.shape[0] / m\n",
    "            \n",
    "            # Mean and variance for each feature in this class\n",
    "            self.means[cls] = np.mean(X_cls, axis=0)\n",
    "            self.variances[cls] = np.var(X_cls, axis=0)        \n",
    "\n",
    "\n",
    "\n",
    "    def gaussian_probability(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Computes Gaussian probability density function for value x.\n",
    "        \"\"\"\n",
    "        eps = 1e-6  # To prevent division by zero\n",
    "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var + eps)\n",
    "        exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "        return coeff * exponent\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for the input data X.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for cls in self.class_priors:\n",
    "                # Start with log prior\n",
    "                log_prob = np.log(self.class_priors[cls])\n",
    "                \n",
    "                # Add log likelihoods for each feature\n",
    "                for idx in range(len(x)):\n",
    "                    mean = self.means[cls][idx]\n",
    "                    var = self.variances[cls][idx]\n",
    "                    prob = self.gaussian_probability(x[idx], mean, var)\n",
    "                    log_prob += np.log(prob + 1e-6)  # add epsilon to avoid log(0)\n",
    "                \n",
    "                class_probs[cls] = log_prob  # Store total log probability for class\n",
    "                \n",
    "            # Pick class with highest log probability\n",
    "            predicted_class = max(class_probs, key=class_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of each class for each sample in X.\n",
    "        \"\"\"\n",
    "        prob_results = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            total_prob = 0\n",
    "            for cls in self.class_priors:\n",
    "                prob = np.log(self.class_priors[cls])\n",
    "                for idx in range(len(x)):\n",
    "                    mean = self.means[cls][idx]\n",
    "                    var = self.variances[cls][idx]\n",
    "                    likelihood = self.gaussian_probability(x[idx], mean, var)\n",
    "                    prob += np.log(likelihood + 1e-6)\n",
    "                class_probs[cls] = np.exp(prob)\n",
    "                total_prob += np.exp(prob)\n",
    "            # Normalize probabilities\n",
    "            for cls in class_probs:\n",
    "                class_probs[cls] /= total_prob\n",
    "            prob_results.append(class_probs)\n",
    "        return prob_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e55ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Multinomial Naive Bayes classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        alpha : Laplace smoothing factor (default=1.0)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha               # Smoothing parameter\n",
    "        self.class_priors = {}           # To store P(C) for each class\n",
    "        self.feature_probs = {}          # To store P(x_j|C) for each feature and class\n",
    "        self.classes_ = None             # Unique class labels (set during fit)\n",
    "        self.V = 0                       # Vocabulary size (number of features)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model on the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        X : ndarray of shape (m, n), where m = number of samples, n = number of features\n",
    "        y : ndarray of shape (m,), target labels\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        m, n = X.shape\n",
    "        self.V = n  # Number of features (vocabulary size)\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Calculate class priors P(C)\n",
    "        for cls, count in zip(self.classes_, class_counts):\n",
    "            self.class_priors[cls] = count / m\n",
    "        \n",
    "        # Calculate likelihoods P(x_j|C) with Laplace smoothing\n",
    "        for cls in self.classes_:\n",
    "            X_cls = X[y == cls]  # Filter samples of this class\n",
    "            feature_counts = np.sum(X_cls, axis=0)  # Sum feature occurrences\n",
    "            total_feature_count = np.sum(feature_counts)\n",
    "            \n",
    "            # Apply Laplace smoothing\n",
    "            probs = (feature_counts + self.alpha) / (total_feature_count + self.alpha * self.V)\n",
    "            \n",
    "            self.feature_probs[cls] = probs  # Store probabilities for this class\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "            \"\"\"\n",
    "            Predict probabilities for each class for the input samples.\n",
    "            \n",
    "            Parameters:\n",
    "            X : ndarray of shape (m, n), input samples\n",
    "            \n",
    "            Returns:\n",
    "            probabilities : ndarray of shape (m, len(classes)), predicted probabilities\n",
    "            \"\"\"\n",
    "            X = np.array(X)\n",
    "            m = X.shape[0]\n",
    "            probabilities = []\n",
    "            \n",
    "            for x in X:\n",
    "                class_log_probs = {}\n",
    "                for cls in self.classes_:\n",
    "                    # Start with log(P(C))\n",
    "                    log_prob = np.log(self.class_priors[cls])\n",
    "                    \n",
    "                    # Add log(P(x_j|C)) weighted by feature counts x_j\n",
    "                    log_prob += np.sum(x * np.log(self.feature_probs[cls] + 1e-9))  # +1e-9 for log(0)\n",
    "                    \n",
    "                    class_log_probs[cls] = log_prob\n",
    "                \n",
    "                # Convert log probabilities to actual probabilities\n",
    "                max_log = max(class_log_probs.values())\n",
    "                exp_probs = {cls: np.exp(log_p - max_log) for cls, log_p in class_log_probs.items()}\n",
    "                total_prob = sum(exp_probs.values())\n",
    "                normalized_probs = [exp_probs[cls] / total_prob for cls in self.classes_]\n",
    "                probabilities.append(normalized_probs)\n",
    "            \n",
    "            return np.array(probabilities)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for the input samples.\n",
    "        \n",
    "        Parameters:\n",
    "        X : ndarray of shape (m, n), input samples\n",
    "        \n",
    "        Returns:\n",
    "        predictions : ndarray of shape (m,), predicted class labels\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3201ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [0 0]\n",
      "Predicted Probabilities:\n",
      " [[0.57142857 0.42857143]\n",
      " [0.96969697 0.03030303]]\n"
     ]
    }
   ],
   "source": [
    "# Example training data\n",
    "X_train = [\n",
    "    [3, 0, 1],\n",
    "    [2, 0, 2],\n",
    "    [0, 4, 0],\n",
    "    [0, 3, 1]\n",
    "]\n",
    "y_train = [1, 1, 0, 0]\n",
    "\n",
    "# Example test data\n",
    "X_test = [\n",
    "    [1, 1, 0],\n",
    "    [0, 2, 1]\n",
    "]\n",
    "\n",
    "# Create and train model\n",
    "model = MultinomialNaiveBayes(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "print(\"Predicted Classes:\", model.predict(X_test))\n",
    "print(\"Predicted Probabilities:\\n\", model.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5baf168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [0 1]\n",
      "Predicted Probabilities: [{0: 0.9999986594824308, 1: 1.3405175691438863e-06}, {0: 1.3405175691438857e-06, 1: 0.9999986594824308}]\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "X_train = np.array([[1.0], [2.0], [1.5], [5.0], [6.0], [5.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "X_test = np.array([[1.2], [5.8]])\n",
    "\n",
    "# Train & predict\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Predicted Classes:\", predictions)\n",
    "print(\"Predicted Probabilities:\", probabilities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
